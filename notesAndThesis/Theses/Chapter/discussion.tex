\graphicspath{{Chapter/Figs/evaluation/}}
\chapter{Discussion}

\section{Evaluation}
\subsection{Classical Approach}
\pic{The resulting landmarks after filtering using different certainty thresholds. $c_\theta=0.01$,  $c_\theta=0.05$ and $c_\theta=0.1$, on three different \ac{fsd} tracks "FSG", "FSI" and "Track 0", $c_\theta=0.05$ has the best results.}{covarianceFiltering}{0.9}$ $\\Anayzing the effect different covariance thresholds have on the quality of the resulting map, it is evident that $c_\theta=0.05$ performs best, while a smaller threshold of $c_\theta=0.01$ filters too many landmarks out resulting in an incomplete map. With a larger threshold of $c_\theta=0.1$, too little noise is filtered out, resulting in over-detections in the map, as seen in figure \ref{fig:covarianceFiltering}.\\
\\
A problem that occurred in developing more advanced means of filtering noisy data is the inability to track landmarks across frames. While it should theoretically possible to pass the information of the change in landmarks over time, the current implementation of the \ac{slam} makes it difficult to do so since it assigns new identifiers to each landmark in each frame. Though, it would be possible to match landmarks based on their position, since the change in position approximates a continuous transformation, given small enough timesteps between frames. This spatial matching, however, is not practical for performance reasons, as it would be computationally expensive. Additionally, the implementation of the particle \ac{slam}s uses different particles to determine the current position in the virtual map of which the one with the highest certainty is chosen to provide the landmarks in the current frame. Because every particle tracks its own landmarks, once the \ac{slam} decides another particle has better certainty, the estimated position as well as all landmarks jump discontinuously to arbitrarily distant locations, which, in turn, makes spatial matching of landmarks impossible. The inability to track landmarks over time also means that it can not be determined which landmarks are newly added in one frame to another.
\subsubsection{Analysis – Deviation From \ac{gt}}
In the following, the figures \ref{fig:workingClassical}, \ref{fig:errorsClassical} and \ref{fig:simulatedTracksClassical} will use the same structure and color scheme. Each figure consists of two parts: the input for the classical algorithm on the left for clarity and the input with the overlayed output of the algorithm on the right. The thin green line represents the ground truth that was used to create artificial track data. Large red- and blue dots represent input cone positions of yellow and blue cones, respectively, where in an effort to increase legibility and visual clarity, yellow cones were replaced by red ones. Large light blue circles represent cones that were filtered out by the covariance filter. The radius of the circle represents the uncertainty of their respective cone. Black dots represent cone misdetections where no color was detected for certain. Grey dots represent cones that were not detected at all, thus marking the error of non-detection stated previously. The output of the algorithm is visualized in three parts: The large green dots represent the points of the calculated centerline, the light blue and small light red points represent the two sets of cones that are the result of preprocessing the cones, namely spatially ordered and readded missing cones. The arrows connecting these three parts mark the order in which these lists of points are contained in the output arrays. This ordering can be used to see the orientation (clockwise/counter-clockwise) of these lists. 
\pic{Examples where the classical approach works well: Unaltered artificially created data on the left and artificially created data with non-detections and misdetections with no color certainty on the right}{workingClassical}{1}\\
While the centerline generally works notably well with perfect data, it starts to produce less and less usable output with increasing errors in the input data. Error-free artificially created data, as well as artificially created data with several non-detections and misdetections where no color is identified, is handled well (figure \ref{fig:workingClassical}). 

Since no reassignment of detected colors is done and the algorithm assumes a closed loop as track, wrongly detected colors and partial tracks impose an immediate problem that is unhandled by the algorithm (figure \ref{fig:errorsClassical}).
\pic{Anormalies the classical approach can not handle: cones with misdetected certain colors on the right and non closed loops on the left}{errorsClassical}{1}  
\\Appliying the algorithm to simulated \ac{slam} data archives mixed results depending on the particularities of a certain frame, as exemplified in figure \ref{fig:simulatedTracksClassical}. More precisely, in frames where there happen to be little to no misdetected colors, the algorithm performs well, detecting the centerline perfectly except for where the misdetected colors are. However, in frames where over-detections and misdetected colors are more present, the orientation of the cones can not be determined correctly, which leads to a centerline, which is unusable in major parts of the track. 
\pic{The classical approach applied to simulated track data. In the upper example, no color midsections are present and over-detections are rare, which makes the algorithm perform well. In the middle example, over-detections and color misdetections lead to a misjudgment of the orientation, which breaks major parts of the centerline. In the bottom example, two color misdetections break the centerline at these places.}{simulatedTracksClassical}{1} 

\subsection{Machine Learning Approach}
The neural network for the \ac{ml} approach was trained using a mean average loss with ADAM as optimization algorithm and a learning rate of $0.001$. These parameters were chosen heuristically and were proven to be sufficient. The loss converged quickly and only 15 epochs with a batch size of 2 were already enough to archive the most optimal validation loss while preventing overfitting. Additionally, a low number of training samples (2000) lead to similar results in validation as 15000 and 20000 did. These settings lead to an average test loss of $0.112$.
\pic{The training and validation loss after 15 epochs with a batch size 2, 1400 training samples and 600 validation samples, ADAM with mean average loss, and $0.001$ learning rate}{training}{1}

\subsubsection{Analysis - Driving Test}
Since the average loss alone is not very expressive in describing the usability of the neural network, it can be evaluated by letting a driver test the curvature predictions made by the algorithm. To archive this, a simple test implementation of the 4th stage of Rosyard pipeline can be used, which applies a steering that is proportional to the estimated curvature $\kappa$. In the test implementation, a proportionality constant of $k = 240$ in addition to a low-pass filter is used to smooth out rapid changes in steering, which is realized by exponential smoothing with a smoothing factor of $\alpha = 0.8$. This leads to the overall formula for the steering for the timestep $t$ as follows:\\
$$steering_0= k\kappa$$
$$steering_t= \alpha k\kappa + (1-\alpha)*steering_{t-1}, t > 0$$ 

\pic{Track 1 being driven autonomously by the machine learning approach and a simple test driver. On the right, a moderate right turn can be seen along the image the \ac{nn} sees. On the left an overview of the track can be seen while the car drives a moderate left turn along the image the \ac{nn} sees.}{mlDriving}{1}
$ $\\Even though the neural network was trained only using training samples where the driver is centered on the track, a deviation from the centerline is interpreted beneficially as the curve leaning towards the opposite direction, which forms a feedback loop that keeps the car in the middle of the track (figure \ref{fig:centerDeviation}).
\pic{The same image fed into the \ac{nn} with three different x offsets: centered, positive offset, and negative offset. The effect a deviation from the track center has on the prediction of curvatures makes the model predict a curve that lets the car return to the center in a closed feedback loop.}{centerDeviation}{0.65} 

Notably, this allows the car to stay on the track and drive laps completely without crossing the boundaries of the track. The approximative nature of a CNN allows it to drive even using very noisy data by finding a suitable approximation for the local centerline instead of solving the centerline exactly. This allows the \ac{cnn} to drive the first lap without any prior data, however, the curvatures produced by the \ac{cnn} cannot be directly used to create a full centerline for the whole track, as only the local centerline directly in front of the car is approximated. This local approximation is, by definition, guaranteed to start at the position of the car, which might not be the center of the track. Lastly, the \ac{cnn} cannot benefit from data about parts of the track that are not immediately ahead of the car.


\section{Comparison of Approaches}

The classical and machine learning approach both solve different problems and work well in their respective domain. The classical algorithm produces accurate output when used with input with enough certainty and a low level of noise. It serves the goal of computing a complete precise centerline for the whole track. However, when used with too noisy data, it fails to detect the centerline completely, which is the case with the noise level the current \ac{slam} implementation and preprocessing provides. The machine learning approach is much more noise-tolerant and usable without any contextual knowledge about the track. Hence, it can be used in the first lap. It is more robust against erroneous data but produces only local and overall less precise centerline points. Thus, less planning ahead is possible and more work is needed to generate a complete map after completing a full lap.

Overall, the classical approach produces precise centerline data when provided with the right data, but is too fragile resulting in a failure to produce usable output consistently when used with simulated data, thereby rendering it – as is – not applicable to be used to drive the car. The machine learning approach on the other hand, while producing much less precise output, is noise resilient enough to successfully drive the car around the track.

\subsection{Runtime}
Another noteworthy advantage the \ac{ml} approach has over the classical approach is its runtime. An average track contains around 200 cones, of which on average half need to be processed by either algorithm to produce a centerline. The start-to-finish runtimes of a complete frame in either algorithm is compared in the following table \ref{table:2}. The system used to measure the runtimes runs on an Intel® Core™ i7-5820K and NVIDIA GeForce GTX 980 Ti. The following runtimes were obtained over an average of 50 frames:\\\\
\begin{table}[h!]
\centering
\begin{tabular}{ |p{1.5cm} p{3cm}||p{4cm}|p{5cm}|  }
    \hline
    \multicolumn{4}{|c|}{Runtime per frame} \\
    \hline
    Track  & Number of Cones  & Classical Algorithm & Machine Learning Algorithm\\
    \hline
    \hline

    \multirow{3}{*}{FSD} & 50  & 171 ms & 63 ms \\
                         & 100 & 910 ms & 61 ms\\
                         & 185 & 1948 ms & 66 ms\\ 
                         \hline
    \multirow{3}{*}{FSI} & 50  & 93 ms & 63 ms\\
                         & 101 & 622 ms & 66 ms\\
                         & 182 & 2397 ms & 65 ms\\ 
                         \hline
    \multirow{3}{*}{Track 0} & 50  & 330 ms & 61 ms\\
                             & 101 & 927 ms & 63 ms\\
                             & 269 & 5540 ms & 68 ms\\ 
                             \hline
   \end{tabular}
\caption{Runtimes per frame for the classical algorithm and machine learning algorithm on different tracks with a different number of cones present}
\label{table:2}
\end{table}
$ $\\
The runtimes of the classical algorithm imply a cubic runtime in regard to the number of cones presented. The machine learning approach implies a constant runtime. Additionally, the \ac{ml} approach in the average case of about 100 cones takes an order of magnitude less time to execute per frame, while a constant execution time also helps to compute data reliably, regardless of how long the car has been running. The classical algorithm will eventually need to discard older data to maintain a constant execution time per frame. Currently, it is able to maintain about 1.6 \ac{fps} in the average case, 5 \ac{fps} in the best case and 0.5 \ac{fps} in the worst case. The machine learning algorithm was able to maintain 16 \ac{fps} consistently. 