

\graphicspath{{Chapter/Figs/FoundationsAndTechnologies/}}
\chapter{Foundations and Technologies}

\section{Raceyard and Formula Student}
Formula Student is a global competition for building racing cars. The subclass \ac{fsd} is focused on autonomous driving and is divided into different disciplines, whereof Autocross is the most relevant for this thesis. The goal in Autocross is to drive a previously unknown track for one lap as fast as possible, thus all data about the track must be gathered and processed in realtime with no prior map.
Since 2005, Raceyard is Kiel's Formula Student team and aims to compete in \ac{fsd} in upcoming competitions.

\pic{The T-Kiel A CE, one of Raceyard's latest cars (Source: https://raceyard.de/autos/, accessed 18.11.2021)}{raceyard_car}{0.75}

\subsection{The Rosyard Pipeline}
The software to be used in \ac{fsd} by the Raceyard car is called \emph{Rosyard} and is built on the \ac{ros} \cite{ros}.
In \ac{ros}, processing takes place in \emph{nodes}, which can communicate with each other using data channels refered to as \emph{topics}. The nodes can be written in Python or C++ and are connected in a way that forms a pipeline in a feedforward fashion. The pipeline processes sensory data as input to compute data that can be used to move the actuators of the vehicle as output. The pipeline consists of five stages which are each represented by one or more nodes plus sensory input:
\begin{enumerate}
    \item Input/Detection: sensory input from cameras and \ac{imu}, also preprocessing
    \item SLAM: extracts landmarks and locates them in a virtual map
    \item Estimation: estimates centerline in the virtual map
    \item Driving: given map data determines the steering and velocity
    \item Lowlevel: hardware controlling
\end{enumerate}
\pic{Visualization of the Rosyard pipeline with stages that contain one or more nodes. Also visualized are the topics that are published and subscribed to by the nodes, represented by messages (Source: adapted from \url{https://git.informatik.uni-kiel.de/las/rosyard/-/blob/master/docu/images/overview.png})}{RosyardPipeline}{1}This thesis focuses on the implementation of the thrid stage. Given the landmarks located in a virtual map from the \ac{slam}, this node should estimate the course of the track, such that the fourth stage can successfully drive the car along the track.
The pipeline is fully dockerized and runs in four different docker containers: the Roscore coordinating everything in \ac{ros}, an optional visualization container, a simulation container for providing fake sensory input, and a container running all pipeline nodes.

\section{Machine Learning}
Machine Learning describes a class of algorithms that have the ability to improve automatically, a process known as learning. The most commonly used types of learning include supervised learning, unsupervised learning and reinforcement learning \cite{Dey2016}. In supervised learning, a set of labeled data, called training data, is used to improve the parameters of the algorithm to make it predict labels better without explicit programming. Supervised learning can be used to train artificial neural networks. A \ac{nn} can be modeled as a directed graph consisting of artificial neuron as nodes and connection between neurons as edges. One example for artificial neurons are perceptrons, which are an abstract and mathematically easy to compute model of a biological neuron. A perceptron receives a number of inputs $x$ and by using the weights of the inputs $w$ calculates their weighed sum $z=w \cdot x$ and passes it through an activation function $f$. This leads to the output
$y=f(z)$, which is called the activation of the perceptron. Common activation functions include linear $f_{linear}(x)=a \cdot x$ for some factor $a \in \mathbb{R^+}$ (commonly $1$) and \ac{relu} $f_{ReLU}(x)=max(a,x)$ \cite{Ramachandran2017}, where \ac{relu} can be used to introduce non-linearity.

\subsection{Deep Learning and Multilayer Perceptions}
\pic{A schematic diagram of a Multi-Layer Perceptron (MLP) neural network.  (Source: Figure 5, An Oil Fraction Neural Sensor Developed Using Electrical Capacitance Tomography Sensor Data, Khursiah Mokhtar, 2013)}{mlp}{0.75}
Multiple Perceptrons can be arranged in layers to form a special kind of \ac{nn}, called \ac{mlp}. In such a layer, a perceptron may only have connections to perceptrons in the preceding layers. A layer that has the maximum number of connections to the previous layer, such that each neuron is connected to every neuron in the previous layer, is called a \emph{fully connected layer}. An \ac{mlp} consists of an input layer, an output layer, and a variable number of so-called hidden layers in between the input and output layer. By having at least two hidden layers, the decision boundary of an \ac{mlp} can take an arbitrary form, allowing it, in theory, to solve arbitrarily complex problems as opposed to a single perceptron, which can only solve linearly separable problems \cite{Lapedes1988}. In recent years, research primarily focuses on networks with an even greater number of hidden layers. Such networks with a large number of layers are called deep networks. Deep networks commonly use non-linear activation functions which results in the optimal weights not being able to be found analytically. Therefore, other algorithms for learning are required; so-called \emph{deep learning}. One of those algorithms is backpropagation, which uses gradient descent to learn the weights as an optimization problem of the weights with respect to the desired output. Usually, deep networks do not contain only fully connected layers, because these layers would introduce unnecessary complexity in the number of trainable parameters. Also, through many consecutive activations, the effect of errors is increasingly amplified or diminished by the intermediate non-linear activation functions, thus creating either a vanishing or exploding effect which hinders efficient learning \cite{Hanin2018}.

\subsection{Convolutional Neural Networks}
In \ac{cnn}s, the concept of \ac{mlp}s is extended by adding convolutional- and pooling layers in a neural network. Convolutional layers allow for processing a large number of inputs while not imposing a huge number of learnable parameters as a fully connected layer would. As a result of having this property, convolutional layers are ideal for processing images, as even small images like 32x32 RGB images already have 3072 inputs.
\pic{Architecture of a \ac{cnn}. An image as input is fed through multiple convolutional layers and pooling layers. The output of these Layers is flattened and fed into fully connected layers to compute the output. (Source: Deep Learning model-based Multimedia forgery detection, Pratik Kanani,  2020)}{cnn}{0.75}
A convolutional layer uses a number of weights matrices, called kernels, of a fixed small size (e.g. 5x5). These kernels are convolved across the width and height of the inputs, meaning the dot product of the filter and a specific local region is computed for each input, thereby computing a two-dimensional map of that kernel. The weights of the kernels can be learned using backpropagation, while certain hyperparameters must be set when designing the \ac{nn}. One of such parameters can be the size and number of kernels used, as well as another called \emph{stride}, which describes how many pixels the kernel is "moved" after each calculation, skipping pixels as center for the kernel. Around the edges, the input needs to be padded (usually with zeros), so that the edges of the input can be processed as well.
A pooling layer reduces the number of inputs by partitioning the input along the width and height into equally sized chunks (e.g. 2x2) and computing an output for each of these chunks. Some commonly used pooling is max pooling, calculating the maximum of its inputs, and average pooling, calculating the arithmetical mean.
Often, convolutional- and pooling layers are succeeded by fully connected layers, which are then used to compute the final output of a network.




%\subsection{Stochastic Gradient Descent}
%
%\section{Traveling Salesman Problem Approximation}
%\ac{tsp}

\section{Discrete Curvature}

\picWrap{A polyline over the vertices $P_0$ to $P_6$}{polyline}{0.25}{R}

Discrete curvature applies the concept of curvature from a continuous curve to a discrete curve called a polyline.\\
A polyline is a series of line segments and is determined by a sequence of points $(P_0,...,P_n)$ $n \in \mathbb{N}$ where each line segment connecting a pair of adjacent points $[P_i,P_{i+1}]$ $i \in \mathbb{N}_{\le n}$ forms a vertex in the polyline.\\
\\In the continuum the curvature $\kappa$ in a point of a differentiable curve is defined by the radius of the osculating circle in that point. This definition, however, is not useful to determine the curvature in a (discrete) polyline, given its non-differentiable nature. All straight segments would have a curvature of $0$ while the curvature in the edges would diverge to infinity. A new definition must be used to determine the curvature of a series of line segments, which can then in turn be used to approximate this series. A different definition can be derived from the quotient of the circular angle $\varphi$ and the arc length $s$:
$$\kappa  =  {\frac{{d\,\varphi}}{{ds}}}$$
Using this idea, we can define the curvature from a point $A$, a heading  $\vec h$ in that point and a point $B$ as the reciprocal of the radius of the circle passing through $A$ and $B$ and being tangent to  $\vec h$ in $A$.
\pic{Points $A$ with heading $\vec h$ and $B$ in circle with radius $r$, implying a curvature in point $A$ of $1/r$. The circle center and $B$ and $A$ form an isosceles triangle with base angle $\gamma$ and vertex angle $\beta$}{discretecurvature}{0.5} \\ Now, we can calculate the curvature $\kappa$ as the reciprocal of the radius of this circle as follows:

Since $\vec h$ is tangent it follows: $$\gamma=90^{\circ} - \alpha$$ and $$180^{\circ}=2\gamma+\beta$$ thus $$(1) \space \beta = 2 \alpha$$
Generally, the length of the secant of a circle $s:=|\vec{AB}|$ can be calculated as
$s = 2r \cdot  sin(\frac{\beta}{2})$. Together with (1) we can derive:
$$\frac{1}{r} = \frac{2sin(\alpha)}{s} = \frac{2sin(\measuredangle(\vec{AB},\vec h))}{|\vec{AB}|} = \kappa$$\\

\picWrap{Example curvature of $1/r$ approximating a polyline leading from $A$ to $B$, the circle corresponding to the curvature has the radius $r$}{curvatureExample}{0.25}{L}Using this method, we can calculate the average curvature of the curve that is tangent in $A$ to $\vec h$ and passing through $B$. This approximates the polyline connecting these points using the points $A$, $B$ and the heading  $\vec h$, which can be derived from $A$ and the next point after $A$ leading to $B$.
Doing this for differently distant points $B$ on a polyline gives us a suitable approximation for the course of a polyline starting from point $A$. Hereby, the shape of the polyline is negelcted completely, which fails to detect S-curves between point $A$ and $B$. It does, however, impose no problem if we choose a fairly small distance betweet point $A$ and $B$, in a way that the variance of the curvature for intermediate points is non-significant.

\section{Simultaneous Localization and Mapping}
\ac{slam} algorithms solve the chicken-and-egg problem of localizing an agent in a map and mapping the environment surrounding an agent. Since for localization a map is seemingly needed and for creating a map of the surrounding the position of an agent needs to be known, the natural solution is to solve both simultaneously. While an exact solution is often not possible  or desirable computation cost wise, several methods exist that can approximate the problem. These approximations for example use \ac{ekf}, graphs, or particle filters. The \ac{slam} used as input for the approaches in this thesis is an implementation of FastSLAM \cite{FastSLAM2002}, which is based on particle filters. In FastSLAM, particles are used as potential positions for the agent. At each timestep, a weight is assigned to the particles according to their likelihood of being consistent with the sensed nearby landmarks. Next, new particles are created according to the spatial distribution of weight, thereby converging to the actual position. In any timestep, the particle with the biggest weight is guessed as the actual current position and reported as such. This leads to the problem of  discontinuity in the virtual space, when the particles diverge to two or more different positions and the previous most likely position becomes less likely than another distant position. When this occurs, the generated map along the estimated position jump in a discontinuous manner. This also imposes the problem that landmarks cannot be identified consistently across time, since every particle keeps track of its own landmarks. Once the estimated position shifts discontinuously, the landmarks cannot be associated to the previous landmarks, because the transformation is discontinuous. \\The output of FastSLAM is the incrementally built map of landmarks in relation to the estimated position of the agent. The landmarks have an uncertainty in the x- and y-dimension associated with them in the form of a covariance matrix. This can later be used to filter for accidental detection of landmarks.

\section{Development Environment Used}
\pic{Screeshots of the prototyping environment coded using web technologies that was used to develop and test the implementation of this thesis. In green, a button can be seen that invokes Python code in the browser to calculate the centerline of the currently drawn or loaded track.}{devEnvironment}{0.75}For developing and testing the implementation of the approaches, web technologies were used as the development environment, as this allows for fast prototyping and easy building of a visual interface and visual output. Additionally, this makes the prototypes easily sharable as they can be hosted on a web server and accessed via browser. Specifically the JavaScript model-view-viewmodel framework vue.js\footnote{\url{https://vuejs.org/}} was used. Since the main source code of Raceyard, as well as the previous algorithm, is written in Python, the ability to run Python code was crucial for the development. While one possibility was the usage of a dedicated server running Python code with specific parameters that reports the result back to the web application, a web integrated solution would be more desirable.

\subsection{Pyodide}
Pyodide is a port of CPython to WebAssembly \cite{pyodide} which allows the execution of Python code directly within a browser using WebAssembly. As opposed to other systems, Pyodide does not cross-compile Python to JavaScript, but uses a Python runtime to execute Python code on demand. Further, many of the most frequently used scientific Python libraries, e.g. NumPy, SciPy, Pandas and Mathplotlib, are supported without extension, which makes it usable for many Python scripts without modification. The non-native execution, however, comes at a performance cost of running at about 2x to 10x slower than native Python, depending on the amount of C code used in packages \cite{pyodide2021}\cite{Jangda2019}. Adding Pyodide to the development environment allowed the web application to be served completely statically, meaning it could be published on a static website hosting service such as GitHub Pages\footnote{\url{https://dsalex1.github.io/BachelorThesisRaceyard/}}.